{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Recommendation on MathOverflow\n",
    "\n",
    "To run this notebook, you'll need to download (this MathOverflow dump)[https://s3-eu-west-1.amazonaws.com/lateral-datadumps/mathoverflow.tar.gz] (56MB, compressed).  We assumed that you've de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import random\n",
    "from featuriser import Featuriser, TextAnalyser, TagAnalyser\n",
    "from indexed_sparse_matrix import IndexedSparseMatrix as ISM\n",
    "from fm import FM, CYTHON_FLOAT, CYTHON_UINT, EntityModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 50\n",
    "margin = 0.85\n",
    "learning_rate = 1\n",
    "max_sampled = 50\n",
    "min_interaction_count = 4\n",
    "min_document_frequency = 5\n",
    "feature_idf = True\n",
    "epochs = 5\n",
    "number_threads = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hstack_ism(rows, isms):\n",
    "    \"\"\"\n",
    "    A wrapper for scipy's hstack to avoid the blocks=[] corner case.\n",
    "    `ism` is an iterable of IndexedSparseMatrix instances.\n",
    "    `rows` is an iterable of row ids.\n",
    "    Returns an IndexedSparseMatrix (CSR) with dtype CYTHON_FLOAT, and a list of\n",
    "    pairs defining the ranges of the input matrices in the stacked one.\n",
    "    Syncs the row indices of each of the IndexedSparseMatrix with `rows` before\n",
    "    stacking.  Column names are combined with the 0-offset index of their\n",
    "    original IndexedSparseMatrix in a tuple e.g. (2, \"banana\").\n",
    "    \"\"\"\n",
    "    if len(isms):\n",
    "        blocks = []\n",
    "        cols = []\n",
    "        for i, ism in enumerate(isms):\n",
    "            ism.sync_row_index(rows)\n",
    "            blocks.append(ism.M)\n",
    "            cols.extend([(i, col_id) for col_id in ism.cols])\n",
    "        ends = np.cumsum([b.shape[1] for b in blocks]).tolist()\n",
    "        starts = [0] + ends[:-1]\n",
    "        ranges = list(zip(starts, ends))\n",
    "        csr_mat = sp.hstack(blocks, format='csr', dtype=CYTHON_FLOAT)\n",
    "        return ISM(csr_mat, rows=rows, cols=cols), ranges\n",
    "    else:\n",
    "        csr_mat = sp.csr_matrix((len(rows), 0), dtype=CYTHON_FLOAT)\n",
    "        return ISM(csr_mat, rows=rows, cols=[]), []\n",
    "\n",
    "    \n",
    "def extract_test_interactions(interactions, N):\n",
    "    \"\"\"\n",
    "    Given a 0/1 valued COO-matrix interactions, create and return a new\n",
    "    COO-matrix containing N of the non-zero entries, randomly chosen.  These\n",
    "    values are subtracted from `interactions`.\n",
    "    Return dtypes and shapes are those of `interactions`.\n",
    "    \"\"\"\n",
    "    indices = random.sample(range(len(interactions.M.data)), N)\n",
    "    for index in indices:\n",
    "        interactions.M.data[index] = 0\n",
    "    test_data = np.ones(len(indices), dtype=interactions.M.dtype)\n",
    "    test_row = [interactions.M.row[i] for i in indices]\n",
    "    test_col = [interactions.M.col[i] for i in indices]\n",
    "    test_coo = sp.coo_matrix((test_data, (test_row, test_col)),\n",
    "                             dtype=interactions.M.dtype,\n",
    "                             shape=interactions.M.shape)\n",
    "    return ISM(test_coo, rows=interactions.rows, cols=interactions.cols)\n",
    "\n",
    "\n",
    "def count_occurrences_ism(ism, axis):\n",
    "    \"\"\"\n",
    "    Given a IndexedSparseMatrix instance wrapping a COO matrix, return a\n",
    "    dictionary mapping each id on the specified axis to the number of nonzero\n",
    "    entries in its row/column.\n",
    "    \"\"\"\n",
    "    ids = [ism.rows, ism.cols][axis]\n",
    "    counts = count_occurrences_coo(ism.M, axis)\n",
    "    return dict(zip(ids, counts))\n",
    "\n",
    "\n",
    "def count_occurrences_coo(coo, axis):\n",
    "    \"\"\"\n",
    "    Given a COO matrix, return a 1d numpy array mapping each index on the\n",
    "    specified axis to the number of nonzero entries in its row/column.\n",
    "    Dtype of returned array is np.int32.\n",
    "    \"\"\"\n",
    "    length = coo.shape[axis]\n",
    "    indices = [coo.row, coo.col][axis]\n",
    "    weights = (coo.data != 0)\n",
    "    counts = np.bincount(indices, weights, minlength=length).astype(np.int32)\n",
    "    return counts\n",
    "\n",
    "\n",
    "def featurise_ids(all_ids, feature_ids):\n",
    "    \"\"\"\n",
    "    Given lists of distinct ids `all_ids` and `feature_ids`, where\n",
    "    `feature_ids` is a subset of `all_ids`, return a IndexedSparseMatrix (CSR)\n",
    "    of dtype CYTHON_FLOAT.\n",
    "    Rows are enumerated by `all_ids`, columns by `feature_ids`, where all\n",
    "    entries are 0/1, and 1 iff the id of the row is in `feature_ids`.\n",
    "    \"\"\"\n",
    "    feature_mapper = dict(zip(feature_ids, range(len(feature_ids))))\n",
    "    shape = (len(feature_ids),)\n",
    "    col_indices = np.empty(shape, dtype=CYTHON_UINT)\n",
    "    row_indices = np.empty(shape, dtype=CYTHON_UINT)\n",
    "    data = np.ones(shape, dtype=CYTHON_FLOAT)\n",
    "    data_no = 0  # number of the COO tuples have we specified so far\n",
    "    for row_no, _id in enumerate(all_ids):\n",
    "        if _id in feature_mapper:\n",
    "            row_indices[data_no] = row_no\n",
    "            col_indices[data_no] = feature_mapper[_id]  # i.e. the column number\n",
    "            data_no += 1\n",
    "    shape = (len(all_ids), len(feature_ids))\n",
    "    csr_mat = sp.csr_matrix((data, (row_indices, col_indices)), shape=shape)\n",
    "    return ISM(csr_mat, rows=all_ids, cols=feature_ids)\n",
    "               \n",
    "               \n",
    "def report_percentile_rank(fm, test_interactions):\n",
    "    print('calculating mean percentile rank')\n",
    "    prs = fm.percentile_ranks(test_interactions.M.row,\n",
    "                              test_interactions.M.col)\n",
    "    stats = (np.mean([pr.pr for pr in prs]),\n",
    "             len(prs),\n",
    "             len(test_interactions.M.row))\n",
    "    print('MPR=%.5f from %i / %i recognisable test interactions' % stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load interaction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lip.tools.indexed_sparse_matrix import IndexedSparseMatrix,\\\n",
    "    value_ordered_keys\n",
    "from collections import defaultdict\n",
    "\n",
    "def build_interaction_matrix(pairs):\n",
    "    \"\"\"\n",
    "    `pairs` is a list of (user_id, document_id) pairs, representing interactions.\n",
    "    \"\"\"\n",
    "    print('building interaction matrix')\n",
    "\n",
    "    # define an enumeration of the ids\n",
    "    user_mapper = defaultdict(lambda: (len(user_mapper), 0))\n",
    "    document_mapper = defaultdict(lambda: (len(document_mapper), 0))\n",
    "    \n",
    "    n_interactions = len(pairs)\n",
    "    shape = (n_interactions,)\n",
    "    row_indices = np.empty(shape, dtype=np.int32)\n",
    "    col_indices = np.empty(shape, dtype=np.int32)\n",
    "       \n",
    "    for i, (user_id, document_id) in enumerate(pairs):\n",
    "        row_indices[i] = user_mapper[user_id][0]\n",
    "        col_indices[i] = document_mapper[document_id][0]\n",
    "        \n",
    "    counts = (len(user_mapper), len(document_mapper), n_interactions)\n",
    "    print('%i users, %i documents, %i preferences' % counts)\n",
    "\n",
    "    user_ids = value_ordered_keys(user_mapper)\n",
    "    document_ids = value_ordered_keys(document_mapper)\n",
    "    data = np.ones(n_interactions, dtype=CYTHON_FLOAT)  # interactions data\n",
    "    shape = (len(user_mapper), len(document_mapper))\n",
    "    interactions = IndexedSparseMatrix(\n",
    "        sp.coo_matrix((data, (row_indices, col_indices)),\n",
    "                      shape=shape, dtype=CYTHON_FLOAT),\n",
    "        rows=user_ids, cols=document_ids)\n",
    "    return interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building interaction matrix\n",
      "20490 users, 53591 documents, 613277 preferences\n"
     ]
    }
   ],
   "source": [
    "def load_interactions():\n",
    "    pairs = []\n",
    "    for line in open('dataset/interactions.csv'):\n",
    "        bits = line.strip().split(',')\n",
    "        pairs.append((bits[1], bits[0]))\n",
    "    return pairs\n",
    "\n",
    "interactions = build_interaction_matrix(load_interactions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_interactions = extract_test_interactions(interactions, N=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document feature selection\n",
    "\n",
    "Note: we are ignoring documents for which we have no interaction data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "doc_ids_seen_counts = count_occurrences_ism(interactions, axis=1)\n",
    "\n",
    "def id_text_iter():\n",
    "    with open('dataset/docs-text.csv') as f:\n",
    "        reader = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "        for row in reader:\n",
    "            if row[0] not in doc_ids_seen_counts:\n",
    "                continue\n",
    "            yield row\n",
    "            \n",
    "def id_tag_iter():\n",
    "    with open('dataset/docs-tags.csv') as f:\n",
    "        reader = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "        for row in reader:\n",
    "            if row[0] not in doc_ids_seen_counts:\n",
    "                continue\n",
    "            yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selecting document id features\n",
      "selecting text features\n",
      "31517 word features\n"
     ]
    }
   ],
   "source": [
    "print('selecting document id features')\n",
    "document_id_features = [_id for _id in interactions.cols\n",
    "                        if doc_ids_seen_counts[_id] > min_interaction_count]\n",
    "\n",
    "vocab_params = {'min_interaction_count': min_interaction_count,\n",
    "                'min_document_frequency': min_document_frequency,\n",
    "                'feature_idf': feature_idf,\n",
    "                'dtype': CYTHON_FLOAT}\n",
    "\n",
    "print('selecting text features')\n",
    "text_featuriser = Featuriser(analyzer=TextAnalyser(), **vocab_params)\n",
    "text_featuriser.fit(id_text_iter(), doc_ids_seen_counts)\n",
    "print('%i word features' % len(text_featuriser.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selecting tag features\n",
      "1052 tag features\n"
     ]
    }
   ],
   "source": [
    "print('selecting tag features')\n",
    "tag_featuriser = Featuriser(analyzer=TagAnalyser(), **vocab_params)\n",
    "tag_featuriser.fit(id_tag_iter(), doc_ids_seen_counts)\n",
    "print('%i tag features' % len(tag_featuriser.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selecting user id features\n",
      "8597 user id features\n"
     ]
    }
   ],
   "source": [
    "print('selecting user id features')\n",
    "seen_counts = count_occurrences_ism(interactions, axis=0)\n",
    "user_id_features = [_id for _id in interactions.rows\n",
    "                    if seen_counts[_id] > min_interaction_count]\n",
    "print('%i user id features' % len(user_id_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featurising user ids\n",
      "featurising document ids\n",
      "featurising text\n",
      "featurising tags\n",
      "joining document feature counts\n"
     ]
    }
   ],
   "source": [
    "print('featurising user ids')\n",
    "user_feature_counts = featurise_ids(interactions.rows, user_id_features)\n",
    "\n",
    "def featurise_documents(interactions, id_text_iter, id_tag_iter):\n",
    "    \"\"\"\n",
    "    Build and return the document x document features CSR matrix\n",
    "    (IndexedSparseMatrix) along with a dict mapping each feature type\n",
    "    to a range (start, stop) that specifies which document features are of\n",
    "    this type.\n",
    "    \"\"\"\n",
    "    blocks = {}\n",
    "    print('featurising document ids')\n",
    "    ids = interactions.cols\n",
    "    blocks['ids'] = featurise_ids(ids, document_id_features)\n",
    "    print('featurising text')\n",
    "    blocks['words'] = text_featuriser.transform(id_text_iter)\n",
    "    print('featurising tags')\n",
    "    blocks['tags'] = tag_featuriser.transform(id_tag_iter)\n",
    "    print('joining document feature counts')\n",
    "    feature_types, counts = zip(*blocks.items())\n",
    "    joined_counts, ranges = hstack_ism(ids, counts)\n",
    "    document_feature_ranges = dict(zip(feature_types, ranges))\n",
    "    return joined_counts, document_feature_ranges\n",
    "\n",
    "document_feature_counts, document_feature_ranges = featurise_documents(\n",
    "    interactions, id_text_iter(), id_tag_iter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the factorisation machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting FM model\n",
      "start epoch 0\n",
      "calculating mean percentile rank\n",
      "MPR=0.04396 from 9387 / 10000 recognisable test interactions\n",
      "start epoch 1\n",
      "calculating mean percentile rank\n",
      "MPR=0.03588 from 9387 / 10000 recognisable test interactions\n",
      "start epoch 2\n",
      "calculating mean percentile rank\n",
      "MPR=0.03341 from 9387 / 10000 recognisable test interactions\n",
      "start epoch 3\n",
      "calculating mean percentile rank\n",
      "MPR=0.03238 from 9387 / 10000 recognisable test interactions\n",
      "start epoch 4\n",
      "calculating mean percentile rank\n",
      "MPR=0.03153 from 9387 / 10000 recognisable test interactions\n"
     ]
    }
   ],
   "source": [
    "users = EntityModel(dimension, learning_rate, user_feature_counts.M)\n",
    "items = EntityModel(dimension, learning_rate, document_feature_counts.M)\n",
    "fm =  FM(users, items, margin=margin, max_sampled=max_sampled)\n",
    "\n",
    "print('fitting FM model')\n",
    "for epoch in range(epochs):\n",
    "    print('start epoch %s' % epoch)\n",
    "    fm.learn(interactions.M, number_threads=number_threads)\n",
    "    report_percentile_rank(fm, test_interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
